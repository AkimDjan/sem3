### Немного про кр
Только рукописные материалы, темы 1 , 3-9

### Гибридная схема
У MPI и OMP есть преимущества и недостатки свои 
1) MPI. Позволяет параллельным приложениям работать на кластерах с распределенной памятью. Доступны все ядра.
2) MPI. Использование различных адресных пространств сокращает количество доступной оперативной памяти на один процесс на узле.
3) MPI. Несколько процессов на узле требуют дополнительных ресурсов операционной системы (коммуникационные буфера, работа MPI менеджера и т.п.).
4) MPI. Сложность управления тысячами, десятками и сотнями тысяч процессов.
5) MPI. Сложность программирования

1) OpenMP. Каждой нити исполнения доступна вся оперативная память. 
2) OpenMP. Относительная простота программирования.
3) OpenMP. На одном узле работает быстрее, чем MPI.
4) OpenMP. Возможно использование только в рамках одного узла.
5) OpenMP. Количество ядер на одном узле сильно ограничено.

Для организации параллельной работы между узлами используется MPI. Для организации параллельной работы внутри узла используется OpenMP.
На каждом узле работает только один MPI процесс (или несколько по количеству слотов) . Внутри процесса запускаются нити исполнения по одной на каждом ядре.

#### Возможные преимущества гибридной схемы
1) Меньше процессов MPI для заданного количества ядер.
2) Легче балансировать загрузку.  Динамическое назначение часто проще реализовать через OpenMP.
3) Уменьшение используемой памяти, если реализация использует реплицированные данные. 
4) Уменьшение затрат на общение между исполнителями.
5) Могут быть доступны новые уровни распараллеливания.
6) Возможность выделения потоков для разных задач, например, выделенный коммуникационный поток или параллельный ввод/вывод
7) Улучшение производительности

#### Гибридная схема
1) Дополнительные накладные расходы на создание/разрушение нитей исполнения.
2) Более сложное программирование.
3) Необходимость учета поддержки нитей исполнения в MPI и других библиотеках. 

не меньше чем я просил
меньше - плохо
равно или больше - все хорошо
MPI_THREAD_SERIALIZED

### Рекомендации по решению задач
Нужно взять нашу программу теплопроводности и дальше реализовать для гибридной схемы, используем также 1000 отрезков, 3 узла и по 3 ядра на каждом узле. Время 1 минуты хватает, если все сделали более менее норм то все отлично. У нас есть наша реализация на MPI и OPENMP. Надо вставлять прагмы аккуратно. Нужно сделать так, чтобы у нас временной цикл лежал внутри параллельной области, дальше реализация рассчета глобальной невязки (3 цикла можем распараллелить), считаем глобальный максимум невязки (через массив), затем между узлами mpi_reduce и mpi_broadcast, сначала внутри узла, должно лежать внутри #pragma single. По сути Модифицировать программу реализованной на MPI. В параллельных областях должны появляться общие переменны и приватные, соответствующие области прагмы выставлять.

